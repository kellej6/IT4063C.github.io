---
title: Importing from Different Data Sources
sidebar_position: 2
draft: false
---


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->

We can construct a `DataFrame` from a variety of data sources. The most common data sources are:
  * CSV files
  * Excel files
  * SQL databases
  * JSON files
  * HTML files

In this notes, we see some of those examples in action

## 游닌 From CSV


```python
import pandas as pd
import numpy as np
```


```python
url = "https://raw.github.com/pandas-dev/pandas/main/pandas/tests/io/data/csv/tips.csv"

tips = pd.read_csv(url)
tips.head()
```
    
<HTMLOutputBlock >




```html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>total_bill</th>
      <th>tip</th>
      <th>sex</th>
      <th>smoker</th>
      <th>day</th>
      <th>time</th>
      <th>size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16.99</td>
      <td>1.01</td>
      <td>Female</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10.34</td>
      <td>1.66</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>21.01</td>
      <td>3.50</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>23.68</td>
      <td>3.31</td>
      <td>Male</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>24.59</td>
      <td>3.61</td>
      <td>Female</td>
      <td>No</td>
      <td>Sun</td>
      <td>Dinner</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>
```



</HTMLOutputBlock>

The `.head()` function gets you the first 5 elements of the data frame to show what the data would look like.

You could also use the function `describe()` which return summary statistics about the dataset


```python
tips.describe()
```
    
<HTMLOutputBlock >




```html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>total_bill</th>
      <th>tip</th>
      <th>size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>244.000000</td>
      <td>244.000000</td>
      <td>244.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>19.785943</td>
      <td>2.998279</td>
      <td>2.569672</td>
    </tr>
    <tr>
      <th>std</th>
      <td>8.902412</td>
      <td>1.383638</td>
      <td>0.951100</td>
    </tr>
    <tr>
      <th>min</th>
      <td>3.070000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>13.347500</td>
      <td>2.000000</td>
      <td>2.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>17.795000</td>
      <td>2.900000</td>
      <td>2.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>24.127500</td>
      <td>3.562500</td>
      <td>3.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>50.810000</td>
      <td>10.000000</td>
      <td>6.000000</td>
    </tr>
  </tbody>
</table>
</div>
```



</HTMLOutputBlock>

### Downloading Dataset from Kaggle using `OpenDatasets`

In cases where you're trying to use datasets that are available on online sources like `Kaggle`. You can continue to use the URL directly, if the source makes one available. Or you could use a tool such as `opendatasets`.
`opendatasets` is a Python library for downloading datasets from online sources like `Kaggle` and `Google Drive` using a simple Python command.

The following examples show how you can (DOWNLOAD) the US Elections Dataset available via `Kaggle`. You will be asked to provide your username and authentication API key (that's not the same as your account password).

**To get your Kaggle API Key:**
![Where to get your Kaggle API Key](./assets/kaggle-profile.jpg)

this will download a file named `kaggle.json` that contains your account name and a key that you'll use to authenticate in the following code cell.


```python
import opendatasets as od
dataset_url = 'https://www.kaggle.com/tunguz/us-elections-dataset'
od.download(dataset_url, data_dir='./data')
```

<CodeOutputBlock lang="python">

```
    Skipping, found downloaded files in "./data/us-elections-dataset" (use force=True to force download)
```

</CodeOutputBlock>

Once downloaded, you can Import the CSV file into a data frame.


```python
## import CSv file from local folder into a dataframe
elections = pd.read_csv('./data/us-elections-dataset/1976-2020-president.csv')
elections.head()
```
    
<HTMLOutputBlock >




```html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>year</th>
      <th>state</th>
      <th>state_po</th>
      <th>state_fips</th>
      <th>state_cen</th>
      <th>state_ic</th>
      <th>office</th>
      <th>candidate</th>
      <th>party_detailed</th>
      <th>writein</th>
      <th>candidatevotes</th>
      <th>totalvotes</th>
      <th>version</th>
      <th>notes</th>
      <th>party_simplified</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1976</td>
      <td>ALABAMA</td>
      <td>AL</td>
      <td>1</td>
      <td>63</td>
      <td>41</td>
      <td>US PRESIDENT</td>
      <td>CARTER, JIMMY</td>
      <td>DEMOCRAT</td>
      <td>False</td>
      <td>659170</td>
      <td>1182850</td>
      <td>20210113</td>
      <td>NaN</td>
      <td>DEMOCRAT</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1976</td>
      <td>ALABAMA</td>
      <td>AL</td>
      <td>1</td>
      <td>63</td>
      <td>41</td>
      <td>US PRESIDENT</td>
      <td>FORD, GERALD</td>
      <td>REPUBLICAN</td>
      <td>False</td>
      <td>504070</td>
      <td>1182850</td>
      <td>20210113</td>
      <td>NaN</td>
      <td>REPUBLICAN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1976</td>
      <td>ALABAMA</td>
      <td>AL</td>
      <td>1</td>
      <td>63</td>
      <td>41</td>
      <td>US PRESIDENT</td>
      <td>MADDOX, LESTER</td>
      <td>AMERICAN INDEPENDENT PARTY</td>
      <td>False</td>
      <td>9198</td>
      <td>1182850</td>
      <td>20210113</td>
      <td>NaN</td>
      <td>OTHER</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1976</td>
      <td>ALABAMA</td>
      <td>AL</td>
      <td>1</td>
      <td>63</td>
      <td>41</td>
      <td>US PRESIDENT</td>
      <td>BUBAR, BENJAMIN ""BEN""</td>
      <td>PROHIBITION</td>
      <td>False</td>
      <td>6669</td>
      <td>1182850</td>
      <td>20210113</td>
      <td>NaN</td>
      <td>OTHER</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1976</td>
      <td>ALABAMA</td>
      <td>AL</td>
      <td>1</td>
      <td>63</td>
      <td>41</td>
      <td>US PRESIDENT</td>
      <td>HALL, GUS</td>
      <td>COMMUNIST PARTY USE</td>
      <td>False</td>
      <td>1954</td>
      <td>1182850</td>
      <td>20210113</td>
      <td>NaN</td>
      <td>OTHER</td>
    </tr>
  </tbody>
</table>
</div>
```



</HTMLOutputBlock>

## 游닌 From API
To import data using an HTTP API in Python, you have multiple options for the HTTP client libraries. The one I choose and use here is `requests`. Here I will show you how you can use `requests` to query data from the US Census Data, and from the John Hopkins COVID APIs.

The most basic example is the following:
```python
import requests

x = requests.get('https://w3schools.com/python/demopage.htm')

print(x.text)
```

However, depending on the API provider, we may need to pass additional configurations and options in the request. We'll see that in the following examples

### US Census Data
The US Census Bureau provides machine-readable dataset via a developer API. 

> Developers could use the statistics available through this API to create apps that:
> 1. Show commuting patterns for every city in America.
> 2. Display the latest numbers on owners and renters in a neighborhood someone may want to live in.
> 3. Provide a local government a range of socioeconomic statistics on its population.

Here are [some of the datasets available that you can use](https://www.census.gov/data/developers/data-sets.html)

Each dataset provides technical documentation for the different variables you could get from that Particular API. Here's [an example](https://api.census.gov/data/2020/dec/pl/variables.html)


- [Working with US Census Data - PDF Guide](https://www.census.gov/content/dam/Census/library/publications/2020/acs/acs_api_handbook_2020_ch02.pdf)

To construct a request with `requests` we need to determine the URL we need to send the request to.


```python
import requests

HOST = "https://api.census.gov/data"
year = "2022"
dataset = "cps/basic/apr"
base_url = "/".join([HOST, year, dataset]) # JOIN the variables with a `/` separator https://api.census.gov/data/2022/cps/basic/apr

# The dataset is huge and contains a lot of data, so we'll request a subset of the available variables.
dataset_variables = ["GEDIV","HRMIS","PENATVTY"] 

predicates = {}
predicates["get"] = ",".join(dataset_variables) # JOIN the variables with a `,` separator
predicates["for"] = "state:*"

response = requests.get(base_url, params=predicates)

census_data = pd.DataFrame.from_records(response.json()[1:], columns=response.json()[0])
print(census_data.head())
```

<CodeOutputBlock lang="python">

```
      GEDIV HRMIS PENATVTY state
    0     8     8      303     4
    1     8     8      303     4
    2     8     8       57     4
    3     8     8       57     4
    4     7     2       57     5
```

</CodeOutputBlock>

This [link here](https://api.census.gov/data/2022/cps/basic/apr/variables.html) describes what those variables mean.

Also, you note that even the states are presented with some numerical values. to get the values of those state IDs, I'll send another request to another dataset


```python
import requests

HOST = "https://api.census.gov/data"
year = "2017"
dataset = "acs/acs5"
base_url = "/".join([HOST, year, dataset]) # JOIN the variables with a `/` separator https://api.census.gov/data/2022/cps/basic/apr

dataset_variables = ["NAME"]
predicates = {}
predicates["get"] = ",".join(dataset_variables)
predicates["for"] = "state:*"

r = requests.get(base_url, params=predicates)
states = pd.DataFrame.from_records(r.json()[1:], columns=r.json()[0])
print(states.head())
```

<CodeOutputBlock lang="python">

```
              NAME state
    0  Mississippi    28
    1     Missouri    29
    2      Montana    30
    3     Nebraska    31
    4       Nevada    32
```

</CodeOutputBlock>

You could subset the data recieved to find the State Numerical Code for the `Ohio`


```python
# Find the state code with the name "Ohio"
print(states[states["NAME"] == "Ohio"])
```

<CodeOutputBlock lang="python">

```
        NAME state
    16  Ohio    39
```

</CodeOutputBlock>

or you could just combine and merge the 2 data sets on the state ID, so we end up with a single dataset with all the information we need.


```python
# Merge census_data and states
full_census_data = pd.merge(census_data, states, on="state")
full_census_data.head() # Print the first 5 rows
```
    
<HTMLOutputBlock >




```html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>GEDIV</th>
      <th>HRMIS</th>
      <th>PENATVTY</th>
      <th>state</th>
      <th>NAME</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7</td>
      <td>6</td>
      <td>57</td>
      <td>40</td>
      <td>Oklahoma</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7</td>
      <td>6</td>
      <td>57</td>
      <td>40</td>
      <td>Oklahoma</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7</td>
      <td>6</td>
      <td>57</td>
      <td>40</td>
      <td>Oklahoma</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7</td>
      <td>6</td>
      <td>57</td>
      <td>40</td>
      <td>Oklahoma</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7</td>
      <td>3</td>
      <td>57</td>
      <td>40</td>
      <td>Oklahoma</td>
    </tr>
  </tbody>
</table>
</div>
```



</HTMLOutputBlock>

### John Hopkins Covid API

John Hopkins University | Cornoavirus Resources Center have been collecting and publishing data regarding COVID-19 since day one. 
You can learn more about that and [find official resources for how to use the data here](https://coronavirus.jhu.edu/about/how-to-use-our-data)

I believe someone is publishing those datasets by means of API through the RapidAPI platform. Whether it's an official resource or not, the goal of this exercise is to show you how to work with APIs in general.

You can see a very easy to understand documentation of [how to use the API here](https://rapidapi.com/axisbits-axisbits-default/api/covid-19-statistics/). 



```python
import requests

url = "https://covid-19-statistics.p.rapidapi.com/reports"

querystring = {"region_province":"Ohio","iso":"USA","region_name":"US","q":"US Ohio","date":"2020-04-16"}

headers = {
	"X-RapidAPI-Key": "b6d38dbbd1msh33a9b59e4f6ddefp148a30jsn8bb7487ef097",
	"X-RapidAPI-Host": "covid-19-statistics.p.rapidapi.com"
}

response = requests.request("GET", url, headers=headers, params=querystring)

print(response.json()["data"])

# save json result into a pandas dataframe
covid_data = pd.DataFrame(response.json()["data"])
covid_data.head()
```
    
<HTMLOutputBlock >

    [{'date': '2020-04-16', 'confirmed': 8414, 'deaths': 407, 'recovered': 0, 'confirmed_diff': 620, 'deaths_diff': 45, 'recovered_diff': 0, 'last_update': '2020-04-16 23:30:51', 'active': 8007, 'active_diff': 575, 'fatality_rate': 0.0484, 'region': {'iso': 'USA', 'name': 'US', 'province': 'Ohio', 'lat': '40.3888', 'long': '-82.7649', 'cities': [{'name': 'Adams', 'date': '2020-04-16', 'fips': 39001, 'lat': '38.84541072', 'long': '-83.4718964', 'confirmed': 3, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Allen', 'date': '2020-04-16', 'fips': 39003, 'lat': '40.77285242', 'long': '-84.10802343', 'confirmed': 57, 'deaths': 6, 'confirmed_diff': 6, 'deaths_diff': 2, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Ashland', 'date': '2020-04-16', 'fips': 39005, 'lat': '40.84772277', 'long': '-82.27280781', 'confirmed': 5, 'deaths': 0, 'confirmed_diff': -1, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Ashtabula', 'date': '2020-04-16', 'fips': 39007, 'lat': '41.70860332', 'long': '-80.74830218', 'confirmed': 45, 'deaths': 3, 'confirmed_diff': 9, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Athens', 'date': '2020-04-16', 'fips': 39009, 'lat': '39.33425634', 'long': '-82.04278644', 'confirmed': 3, 'deaths': 1, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Auglaize', 'date': '2020-04-16', 'fips': 39011, 'lat': '40.55998859', 'long': '-84.22421429999999', 'confirmed': 19, 'deaths': 1, 'confirmed_diff': 1, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Belmont', 'date': '2020-04-16', 'fips': 39013, 'lat': '40.01625942', 'long': '-80.9924051', 'confirmed': 59, 'deaths': 3, 'confirmed_diff': 1, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Brown', 'date': '2020-04-16', 'fips': 39015, 'lat': '38.93416837', 'long': '-83.86788395', 'confirmed': 8, 'deaths': 1, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Butler', 'date': '2020-04-16', 'fips': 39017, 'lat': '39.44012838', 'long': '-84.57388716', 'confirmed': 148, 'deaths': 2, 'confirmed_diff': 15, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Carroll', 'date': '2020-04-16', 'fips': 39019, 'lat': '40.578968599999996', 'long': '-81.09178213', 'confirmed': 14, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Champaign', 'date': '2020-04-16', 'fips': 39021, 'lat': '40.13923427', 'long': '-83.76875242', 'confirmed': 6, 'deaths': 1, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Clark', 'date': '2020-04-16', 'fips': 39023, 'lat': '39.91592258', 'long': '-83.78498252', 'confirmed': 23, 'deaths': 0, 'confirmed_diff': 2, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Clermont', 'date': '2020-04-16', 'fips': 39025, 'lat': '39.04847534', 'long': '-84.15375786', 'confirmed': 60, 'deaths': 1, 'confirmed_diff': 11, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Clinton', 'date': '2020-04-16', 'fips': 39027, 'lat': '39.41485808', 'long': '-83.80852286', 'confirmed': 23, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Columbiana', 'date': '2020-04-16', 'fips': 39029, 'lat': '40.76932373', 'long': '-80.78094576', 'confirmed': 136, 'deaths': 10, 'confirmed_diff': 13, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Coshocton', 'date': '2020-04-16', 'fips': 39031, 'lat': '40.30096166', 'long': '-81.91729018', 'confirmed': 16, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Crawford', 'date': '2020-04-16', 'fips': 39033, 'lat': '40.85065156', 'long': '-82.91989099', 'confirmed': 26, 'deaths': 0, 'confirmed_diff': 3, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Cuyahoga', 'date': '2020-04-16', 'fips': 39035, 'lat': '41.424119', 'long': '-81.65918339', 'confirmed': 1331, 'deaths': 42, 'confirmed_diff': 50, 'deaths_diff': 3, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Darke', 'date': '2020-04-16', 'fips': 39037, 'lat': '40.13412966', 'long': '-84.6194517', 'confirmed': 55, 'deaths': 10, 'confirmed_diff': 3, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Defiance', 'date': '2020-04-16', 'fips': 39039, 'lat': '41.32398837', 'long': '-84.49076944', 'confirmed': 12, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Delaware', 'date': '2020-04-16', 'fips': 39041, 'lat': '40.27942393', 'long': '-83.00457058', 'confirmed': 114, 'deaths': 3, 'confirmed_diff': 6, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Erie', 'date': '2020-04-16', 'fips': 39043, 'lat': '41.36796058', 'long': '-82.62904521', 'confirmed': 19, 'deaths': 1, 'confirmed_diff': 3, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Fairfield', 'date': '2020-04-16', 'fips': 39045, 'lat': '39.75107189', 'long': '-82.63088163', 'confirmed': 96, 'deaths': 2, 'confirmed_diff': 4, 'deaths_diff': 1, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Fayette', 'date': '2020-04-16', 'fips': 39047, 'lat': '39.56021306', 'long': '-83.4562016', 'confirmed': 12, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Franklin', 'date': '2020-04-16', 'fips': 39049, 'lat': '39.96995815', 'long': '-83.01115755', 'confirmed': 1212, 'deaths': 19, 'confirmed_diff': 110, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Fulton', 'date': '2020-04-16', 'fips': 39051, 'lat': '41.60213491', 'long': '-84.12571393', 'confirmed': 10, 'deaths': 0, 'confirmed_diff': 2, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Gallia', 'date': '2020-04-16', 'fips': 39053, 'lat': '38.82708533', 'long': '-82.31647569', 'confirmed': 6, 'deaths': 1, 'confirmed_diff': -2, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Geauga', 'date': '2020-04-16', 'fips': 39055, 'lat': '41.49952319', 'long': '-81.17935342', 'confirmed': 92, 'deaths': 4, 'confirmed_diff': 10, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Greene', 'date': '2020-04-16', 'fips': 39057, 'lat': '39.69116283', 'long': '-83.89032084', 'confirmed': 33, 'deaths': 2, 'confirmed_diff': 2, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Guernsey', 'date': '2020-04-16', 'fips': 39059, 'lat': '40.05026529', 'long': '-81.49248905', 'confirmed': 10, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Hamilton', 'date': '2020-04-16', 'fips': 39061, 'lat': '39.19673558', 'long': '-84.54502924', 'confirmed': 615, 'deaths': 29, 'confirmed_diff': 21, 'deaths_diff': 2, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Hancock', 'date': '2020-04-16', 'fips': 39063, 'lat': '41.00250487', 'long': '-83.66838948', 'confirmed': 25, 'deaths': 1, 'confirmed_diff': 2, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Hardin', 'date': '2020-04-16', 'fips': 39065, 'lat': '40.66015414', 'long': '-83.65929931', 'confirmed': 12, 'deaths': 0, 'confirmed_diff': 1, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Harrison', 'date': '2020-04-16', 'fips': 39067, 'lat': '40.29380509', 'long': '-81.09068544', 'confirmed': 1, 'deaths': 0, 'confirmed_diff': 1, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Henry', 'date': '2020-04-16', 'fips': 39069, 'lat': '41.333964200000004', 'long': '-84.06830637', 'confirmed': 2, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Highland', 'date': '2020-04-16', 'fips': 39071, 'lat': '39.1839264', 'long': '-83.60331456', 'confirmed': 7, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Hocking', 'date': '2020-04-16', 'fips': 39073, 'lat': '39.49537927', 'long': '-82.47991446', 'confirmed': 4, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Holmes', 'date': '2020-04-16', 'fips': 39075, 'lat': '40.56163713', 'long': '-81.92635677', 'confirmed': 3, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Huron', 'date': '2020-04-16', 'fips': 39077, 'lat': '41.14651175', 'long': '-82.59867951', 'confirmed': 16, 'deaths': 1, 'confirmed_diff': 2, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Jackson', 'date': '2020-04-16', 'fips': 39079, 'lat': '39.01914254', 'long': '-82.61818559999998', 'confirmed': 3, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Jefferson', 'date': '2020-04-16', 'fips': 39081, 'lat': '40.38614126', 'long': '-80.76259514', 'confirmed': 24, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Knox', 'date': '2020-04-16', 'fips': 39083, 'lat': '40.39830217', 'long': '-82.42027563', 'confirmed': 11, 'deaths': 1, 'confirmed_diff': 1, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Lake', 'date': '2020-04-16', 'fips': 39085, 'lat': '41.69710807', 'long': '-81.23676539', 'confirmed': 127, 'deaths': 6, 'confirmed_diff': 3, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Lawrence', 'date': '2020-04-16', 'fips': 39087, 'lat': '38.59743452', 'long': '-82.53466552', 'confirmed': 19, 'deaths': 0, 'confirmed_diff': 2, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Licking', 'date': '2020-04-16', 'fips': 39089, 'lat': '40.09136236', 'long': '-82.48185785', 'confirmed': 92, 'deaths': 4, 'confirmed_diff': 4, 'deaths_diff': 1, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Logan', 'date': '2020-04-16', 'fips': 39091, 'lat': '40.38996525', 'long': '-83.76784341', 'confirmed': 8, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Lorain', 'date': '2020-04-16', 'fips': 39093, 'lat': '41.29553751', 'long': '-82.15083537', 'confirmed': 221, 'deaths': 9, 'confirmed_diff': 18, 'deaths_diff': 2, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Lucas', 'date': '2020-04-16', 'fips': 39095, 'lat': '41.62101218', 'long': '-83.65468618', 'confirmed': 644, 'deaths': 28, 'confirmed_diff': 48, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Madison', 'date': '2020-04-16', 'fips': 39097, 'lat': '39.89381073', 'long': '-83.40178317', 'confirmed': 29, 'deaths': 3, 'confirmed_diff': 5, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Mahoning', 'date': '2020-04-16', 'fips': 39099, 'lat': '41.01631101', 'long': '-80.77287029', 'confirmed': 512, 'deaths': 42, 'confirmed_diff': 26, 'deaths_diff': 1, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Marion', 'date': '2020-04-16', 'fips': 39101, 'lat': '40.58610662', 'long': '-83.15736305', 'confirmed': 276, 'deaths': 1, 'confirmed_diff': 112, 'deaths_diff': 1, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Medina', 'date': '2020-04-16', 'fips': 39103, 'lat': '41.11770589', 'long': '-81.89986209999998', 'confirmed': 126, 'deaths': 10, 'confirmed_diff': 4, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Meigs', 'date': '2020-04-16', 'fips': 39105, 'lat': '39.09224872', 'long': '-82.0305041', 'confirmed': 2, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Mercer', 'date': '2020-04-16', 'fips': 39107, 'lat': '40.54043046', 'long': '-84.62912742', 'confirmed': 13, 'deaths': 1, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Miami', 'date': '2020-04-16', 'fips': 39109, 'lat': '40.0543329', 'long': '-84.22871271', 'confirmed': 127, 'deaths': 22, 'confirmed_diff': 2, 'deaths_diff': 1, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Monroe', 'date': '2020-04-16', 'fips': 39111, 'lat': '39.72984936', 'long': '-81.08464734', 'confirmed': 2, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Montgomery', 'date': '2020-04-16', 'fips': 39113, 'lat': '39.75394919', 'long': '-84.29050975', 'confirmed': 213, 'deaths': 8, 'confirmed_diff': 2, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Morgan', 'date': '2020-04-16', 'fips': 39115, 'lat': '39.62081738', 'long': '-81.85308173', 'confirmed': 3, 'deaths': 0, 'confirmed_diff': 1, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Morrow', 'date': '2020-04-16', 'fips': 39117, 'lat': '40.52363560000001', 'long': '-82.7892599', 'confirmed': 14, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Muskingum', 'date': '2020-04-16', 'fips': 39119, 'lat': '39.96575964', 'long': '-81.94363275', 'confirmed': 8, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Noble', 'date': '2020-04-16', 'fips': 39121, 'lat': '39.76818851', 'long': '-81.45937183', 'confirmed': 3, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Ottawa', 'date': '2020-04-16', 'fips': 39123, 'lat': '41.53781826', 'long': '-83.0940185', 'confirmed': 17, 'deaths': 0, 'confirmed_diff': 5, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Paulding', 'date': '2020-04-16', 'fips': 39125, 'lat': '41.11676341', 'long': '-84.5801017', 'confirmed': 5, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Perry', 'date': '2020-04-16', 'fips': 39127, 'lat': '39.73508655', 'long': '-82.23804971', 'confirmed': 8, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Pickaway', 'date': '2020-04-16', 'fips': 39129, 'lat': '39.64170392', 'long': '-83.0243386', 'confirmed': 196, 'deaths': 4, 'confirmed_diff': 24, 'deaths_diff': 4, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Pike', 'date': '2020-04-16', 'fips': 39131, 'lat': '39.07634001', 'long': '-83.06769584', 'confirmed': 1, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Portage', 'date': '2020-04-16', 'fips': 39133, 'lat': '41.16793482', 'long': '-81.19735782', 'confirmed': 162, 'deaths': 24, 'confirmed_diff': 4, 'deaths_diff': 2, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Preble', 'date': '2020-04-16', 'fips': 39135, 'lat': '39.7420247', 'long': '-84.64787018', 'confirmed': 20, 'deaths': 1, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Putnam', 'date': '2020-04-16', 'fips': 39137, 'lat': '41.02094889', 'long': '-84.1336111', 'confirmed': 4, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Richland', 'date': '2020-04-16', 'fips': 39139, 'lat': '40.77180308', 'long': '-82.53799609999999', 'confirmed': 48, 'deaths': 1, 'confirmed_diff': 4, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Ross', 'date': '2020-04-16', 'fips': 39141, 'lat': '39.33705391', 'long': '-83.06000914', 'confirmed': 26, 'deaths': 0, 'confirmed_diff': 5, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Sandusky', 'date': '2020-04-16', 'fips': 39143, 'lat': '41.35624126', 'long': '-83.137872', 'confirmed': 14, 'deaths': 2, 'confirmed_diff': 1, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Scioto', 'date': '2020-04-16', 'fips': 39145, 'lat': '38.80270163', 'long': '-82.98907345', 'confirmed': 5, 'deaths': 0, 'confirmed_diff': 1, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Seneca', 'date': '2020-04-16', 'fips': 39147, 'lat': '41.12351311', 'long': '-83.12783209', 'confirmed': 11, 'deaths': 1, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Shelby', 'date': '2020-04-16', 'fips': 39149, 'lat': '40.33163494', 'long': '-84.20258189', 'confirmed': 28, 'deaths': 0, 'confirmed_diff': 1, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Stark', 'date': '2020-04-16', 'fips': 39151, 'lat': '40.81482476', 'long': '-81.36437305', 'confirmed': 204, 'deaths': 21, 'confirmed_diff': 28, 'deaths_diff': 4, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Summit', 'date': '2020-04-16', 'fips': 39153, 'lat': '41.12464734', 'long': '-81.53123079', 'confirmed': 310, 'deaths': 16, 'confirmed_diff': 16, 'deaths_diff': 1, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Trumbull', 'date': '2020-04-16', 'fips': 39155, 'lat': '41.31735028', 'long': '-80.76109643', 'confirmed': 203, 'deaths': 14, 'confirmed_diff': 4, 'deaths_diff': 1, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Tuscarawas', 'date': '2020-04-16', 'fips': 39157, 'lat': '40.44214695', 'long': '-81.47226262', 'confirmed': 29, 'deaths': 0, 'confirmed_diff': 1, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Unassigned', 'date': '2020-04-16', 'fips': 90039, 'lat': None, 'long': None, 'confirmed': 0, 'deaths': 16, 'confirmed_diff': 0, 'deaths_diff': 16, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Union', 'date': '2020-04-16', 'fips': 39159, 'lat': '40.30016061', 'long': '-83.37239023', 'confirmed': 11, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Van Wert', 'date': '2020-04-16', 'fips': 39161, 'lat': '40.8554138', 'long': '-84.59111700000003', 'confirmed': 2, 'deaths': 0, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Warren', 'date': '2020-04-16', 'fips': 39165, 'lat': '39.42581994', 'long': '-84.16557457', 'confirmed': 89, 'deaths': 4, 'confirmed_diff': 4, 'deaths_diff': 1, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Washington', 'date': '2020-04-16', 'fips': 39167, 'lat': '39.45690571', 'long': '-81.49121382', 'confirmed': 53, 'deaths': 5, 'confirmed_diff': 6, 'deaths_diff': 1, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Wayne', 'date': '2020-04-16', 'fips': 39169, 'lat': '40.82925852', 'long': '-81.88844833', 'confirmed': 66, 'deaths': 11, 'confirmed_diff': 10, 'deaths_diff': 1, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Williams', 'date': '2020-04-16', 'fips': 39171, 'lat': '41.56052014', 'long': '-84.58429552', 'confirmed': 6, 'deaths': 1, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Wood', 'date': '2020-04-16', 'fips': 39173, 'lat': '41.36224827', 'long': '-83.62285108', 'confirmed': 67, 'deaths': 5, 'confirmed_diff': 3, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}, {'name': 'Wyandot', 'date': '2020-04-16', 'fips': 39175, 'lat': '40.84339621', 'long': '-83.30734173', 'confirmed': 14, 'deaths': 2, 'confirmed_diff': 0, 'deaths_diff': 0, 'last_update': '2020-04-16 23:30:51'}]}}]





```html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>confirmed</th>
      <th>deaths</th>
      <th>recovered</th>
      <th>confirmed_diff</th>
      <th>deaths_diff</th>
      <th>recovered_diff</th>
      <th>last_update</th>
      <th>active</th>
      <th>active_diff</th>
      <th>fatality_rate</th>
      <th>region</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2020-04-16</td>
      <td>8414</td>
      <td>407</td>
      <td>0</td>
      <td>620</td>
      <td>45</td>
      <td>0</td>
      <td>2020-04-16 23:30:51</td>
      <td>8007</td>
      <td>575</td>
      <td>0.0484</td>
      <td>{'iso': 'USA', 'name': 'US', 'province': 'Ohio...</td>
    </tr>
  </tbody>
</table>
</div>
```



</HTMLOutputBlock>

## 游닌 Using Socrata
The Socrata Open Data API allows you to programmatically access a wealth of open data resources from governments, non-profits, and NGOs around the world. 

### Cincinnati Datasets
For example, [here are a bunch of datasets that you can find about the City of Cincinnati](https://www.opendatanetwork.com/entity/1600000US3915000/Cincinnati_OH/demographics.population.count?ref=search-entity&year=2018)

For you to be able to access any of those datasets, you need to register to get an application token.

![Cincinnati Data Portal](./assets/cin.jpg)



```python
import os
from dotenv import load_dotenv
import pandas as pd
from sodapy import Socrata

load_dotenv(override=True)

# Example authenticated client (needed for non-public datasets):
app_token = os.getenv("SODA_APP_TOKEN")

# client = Socrata("data.cincinnati-oh.gov", None)
client = Socrata("data.cincinnati-oh.gov",app_token)


# First 2000 results, returned as JSON from API / converted to Python list of
# dictionaries by sodapy.
results = client.get("rvmt-pkmq", limit=2000)

# Convert to pandas DataFrame
results_df = pd.DataFrame.from_records(results)
print(results_df.columns)
```

<CodeOutputBlock lang="python">

```
    Index(['address_x', 'latitude_x', 'longitude_x', 'age',
           'community_council_neighborhood', 'cpd_neighborhood',
           'sna_neighborhood', 'crashdate', 'crashseverity', 'crashseverityid',
           'datecrashreported', 'dayofweek', 'gender', 'injuries', 'instanceid',
           'lightconditionsprimary', 'localreportno', 'mannerofcrash',
           'roadconditionsprimary', 'roadcontour', 'roadsurface', 'unittype',
           'typeofperson', 'weather', 'zip', 'roadclass', 'roadclassdesc'],
          dtype='object')
```

</CodeOutputBlock>

## 游닌 From SQL

### About SQL
SQL is a powerful programming language that allows us to interact and save data in Relational Databases. SQL stores data in a table format, consisting of rows representing a number of records and columns corresponding to various features.

There are many categories categories of SQL commands:
1. Data Definition Language (DDL)
   1. `CREATE`, `ALTER`, `DROP`; these are the commands used to define the data structure or the data model.

2. Data Manipulation Language (DML)
   1. `INSERT`, `UPDATE`, `DELETE`; these are the commands used to modify data in existing databases.

3. Data Query Language (DQL)
   1. `SELECT`

4. Data Control Langauge (DCL)
   1. `GRANT`, `REVOKE`; administrative commands to manage user permissions and access

5. Transaction Control Language (TCL)
   1. `COMMIT`, `ROLLBACK`, `BEGIN TRANSACTION`; for managing transaction queries where multiple DML are processed in a single operation. This is more for software developers than it is for data analysts.

While there is a SQL Standard, there are many dialects and implementations of that standard. You'll often hear of `Microsoft SQL Server`, `MySQL`, `PostgreSQL`, ...etc. All of these require that the an instance of the server is hosted on some server or locally.

Another dialect of SQL is called `SQLite`. This is a small, self-contained, serverless relational database system. This means that the entire database would be contained in a file. that can be shared with others. This has pros and cons.

For the purposes of demo-ing how we can use Python to interact with SQL Database, I'm using a sample SQLite database. The [SQLite Tutorial Website](https://www.sqlitetutorial.net/sqlite-sample-database/) offers same sample database for testing and training. 

I'm here demo-ing using the Chinook database in python. This is the database schema for the database tables.

![DB Schema](https://www.sqlitetutorial.net/wp-content/uploads/2015/11/sqlite-sample-database-color.jpg)


First off, I'll download the sqlite database from [https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip](https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip). The downloaded file is zipped so I will need to also unzip that file. (of course we can do that manually, but how savage is that)


```python
from zipfile import ZipFile
from urllib.request import urlretrieve

file_handle, _ = urlretrieve("https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip")
zipfile = ZipFile(file_handle, 'r')
zipfile.extractall(".")
zipfile.close()
```

We'll need to use the `SQLAlchemy` package from the Python Package Index.

Here we define the connection string a sqlite connection to a locally available SQLite Database file `chinook`


```python
from sqlalchemy import create_engine

connection_string = "sqlite:///chinook.db"
engine = create_engine(connection_string)

dbConnection = engine.connect()
```


```python
import pandas as pd

genres_df = pd.read_sql("SELECT * FROM genres;", dbConnection)
genres_df
```
    
<HTMLOutputBlock >




```html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>GenreId</th>
      <th>Name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Rock</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>Jazz</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Metal</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Alternative &amp; Punk</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>Rock And Roll</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>Blues</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>Latin</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>Reggae</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>Pop</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>Soundtrack</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>Bossa Nova</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>Easy Listening</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>Heavy Metal</td>
    </tr>
    <tr>
      <th>13</th>
      <td>14</td>
      <td>R&amp;B/Soul</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>Electronica/Dance</td>
    </tr>
    <tr>
      <th>15</th>
      <td>16</td>
      <td>World</td>
    </tr>
    <tr>
      <th>16</th>
      <td>17</td>
      <td>Hip Hop/Rap</td>
    </tr>
    <tr>
      <th>17</th>
      <td>18</td>
      <td>Science Fiction</td>
    </tr>
    <tr>
      <th>18</th>
      <td>19</td>
      <td>TV Shows</td>
    </tr>
    <tr>
      <th>19</th>
      <td>20</td>
      <td>Sci Fi &amp; Fantasy</td>
    </tr>
    <tr>
      <th>20</th>
      <td>21</td>
      <td>Drama</td>
    </tr>
    <tr>
      <th>21</th>
      <td>22</td>
      <td>Comedy</td>
    </tr>
    <tr>
      <th>22</th>
      <td>23</td>
      <td>Alternative</td>
    </tr>
    <tr>
      <th>23</th>
      <td>24</td>
      <td>Classical</td>
    </tr>
    <tr>
      <th>24</th>
      <td>25</td>
      <td>Opera</td>
    </tr>
  </tbody>
</table>
</div>
```



</HTMLOutputBlock>

Let's add one item to the dataframe


```python
genres_df = pd.concat([genres_df, pd.DataFrame.from_records([{'GenreId': '26', 'Name': 'Arabic Pop'}])], ignore_index=True)
genres_df
```
    
<HTMLOutputBlock >




```html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>GenreId</th>
      <th>Name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Rock</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>Jazz</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Metal</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Alternative &amp; Punk</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>Rock And Roll</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>Blues</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>Latin</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>Reggae</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>Pop</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>Soundtrack</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>Bossa Nova</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>Easy Listening</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>Heavy Metal</td>
    </tr>
    <tr>
      <th>13</th>
      <td>14</td>
      <td>R&amp;B/Soul</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>Electronica/Dance</td>
    </tr>
    <tr>
      <th>15</th>
      <td>16</td>
      <td>World</td>
    </tr>
    <tr>
      <th>16</th>
      <td>17</td>
      <td>Hip Hop/Rap</td>
    </tr>
    <tr>
      <th>17</th>
      <td>18</td>
      <td>Science Fiction</td>
    </tr>
    <tr>
      <th>18</th>
      <td>19</td>
      <td>TV Shows</td>
    </tr>
    <tr>
      <th>19</th>
      <td>20</td>
      <td>Sci Fi &amp; Fantasy</td>
    </tr>
    <tr>
      <th>20</th>
      <td>21</td>
      <td>Drama</td>
    </tr>
    <tr>
      <th>21</th>
      <td>22</td>
      <td>Comedy</td>
    </tr>
    <tr>
      <th>22</th>
      <td>23</td>
      <td>Alternative</td>
    </tr>
    <tr>
      <th>23</th>
      <td>24</td>
      <td>Classical</td>
    </tr>
    <tr>
      <th>24</th>
      <td>25</td>
      <td>Opera</td>
    </tr>
    <tr>
      <th>25</th>
      <td>26</td>
      <td>Arabic Pop</td>
    </tr>
  </tbody>
</table>
</div>
```



</HTMLOutputBlock>

Let's update the database with the new data


```python
genres_df.to_sql('genres', dbConnection, if_exists='replace', index=False)
```

<CodeOutputBlock lang="python">

```
    26
```

</CodeOutputBlock>

Let's see what's in the database table now


```python
df2 = pd.read_sql("SELECT * FROM genres;", dbConnection)
df2
```
    
<HTMLOutputBlock >




```html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>GenreId</th>
      <th>Name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Rock</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>Jazz</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Metal</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Alternative &amp; Punk</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>Rock And Roll</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>Blues</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>Latin</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>Reggae</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>Pop</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>Soundtrack</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>Bossa Nova</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>Easy Listening</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>Heavy Metal</td>
    </tr>
    <tr>
      <th>13</th>
      <td>14</td>
      <td>R&amp;B/Soul</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>Electronica/Dance</td>
    </tr>
    <tr>
      <th>15</th>
      <td>16</td>
      <td>World</td>
    </tr>
    <tr>
      <th>16</th>
      <td>17</td>
      <td>Hip Hop/Rap</td>
    </tr>
    <tr>
      <th>17</th>
      <td>18</td>
      <td>Science Fiction</td>
    </tr>
    <tr>
      <th>18</th>
      <td>19</td>
      <td>TV Shows</td>
    </tr>
    <tr>
      <th>19</th>
      <td>20</td>
      <td>Sci Fi &amp; Fantasy</td>
    </tr>
    <tr>
      <th>20</th>
      <td>21</td>
      <td>Drama</td>
    </tr>
    <tr>
      <th>21</th>
      <td>22</td>
      <td>Comedy</td>
    </tr>
    <tr>
      <th>22</th>
      <td>23</td>
      <td>Alternative</td>
    </tr>
    <tr>
      <th>23</th>
      <td>24</td>
      <td>Classical</td>
    </tr>
    <tr>
      <th>24</th>
      <td>25</td>
      <td>Opera</td>
    </tr>
    <tr>
      <th>25</th>
      <td>26</td>
      <td>Arabic Pop</td>
    </tr>
  </tbody>
</table>
</div>
```



</HTMLOutputBlock>

## 游닌 From MongoDB

If you've taken Contemporary Database, you have already learned about One of the NoSQL Database options, MongoDB.

In the following example, I will show you how you can use Python to Query Data from a MongoDB.

Here we'll need to install the `PyMongo` Package. Depending on the MongoDB instance you're trying to connect to, you will need an extra companion package. You can find out about that in this [link to the PyMongo Documentation](https://pymongo.readthedocs.io/en/stable/installation.html). For instances:
* For AWS, you need to install `pymongo[aws]`
* For `mongodb+srv` connection strings, you'd need to install `pymongo[srv]`

For the MongoDB instance on Mongo Atlas, we need `pymongo[srv]`, `pipenv install 'pymongo[srv]'`


```python
import pandas as pd
from pymongo import MongoClient
import os
from dotenv import load_dotenv

load_dotenv()

pd.set_option('display.max_columns', None) # Otherwise, pandas would not be able to display the full content of each cell


def _connect_mongo(host, username, password, db):
    """ A util for making a connection to mongo """

    mongo_uri = 'mongodb+srv://%s:%s@%s/?retryWrites=true&w=majority"' % (username, password, host)

    conn = MongoClient(mongo_uri)

    return conn[db]


def read_mongo(db, collection, query={}, no_id=True):
    """ Read from Mongo and Store into DataFrame """

    # Make a query to the specific DB and Collection
    cursor = db[collection].find(query)

    # Expand the cursor and construct the DataFrame
    df =  pd.DataFrame(list(cursor))

    # Delete the _id
    if no_id:
        del df['_id']

    return df

username = os.getenv("MONGO_USERNAME")
password = os.getenv("MONGO_PASSWORD")

db = _connect_mongo(host='it4063c.ykv1yjn.mongodb.net', username=username, password=password, db='sample_airbnb')
df = read_mongo(db=db, collection='listingsAndReviews', query={'bedrooms': {'$lt': 2}}, no_id=False)
df.head()
```
    
<HTMLOutputBlock >




```html
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>_id</th>
      <th>listing_url</th>
      <th>name</th>
      <th>summary</th>
      <th>space</th>
      <th>description</th>
      <th>neighborhood_overview</th>
      <th>notes</th>
      <th>transit</th>
      <th>access</th>
      <th>interaction</th>
      <th>house_rules</th>
      <th>property_type</th>
      <th>room_type</th>
      <th>bed_type</th>
      <th>minimum_nights</th>
      <th>maximum_nights</th>
      <th>cancellation_policy</th>
      <th>last_scraped</th>
      <th>calendar_last_scraped</th>
      <th>accommodates</th>
      <th>bedrooms</th>
      <th>beds</th>
      <th>number_of_reviews</th>
      <th>bathrooms</th>
      <th>amenities</th>
      <th>price</th>
      <th>weekly_price</th>
      <th>monthly_price</th>
      <th>cleaning_fee</th>
      <th>extra_people</th>
      <th>guests_included</th>
      <th>images</th>
      <th>host</th>
      <th>address</th>
      <th>availability</th>
      <th>review_scores</th>
      <th>reviews</th>
      <th>first_review</th>
      <th>last_review</th>
      <th>security_deposit</th>
      <th>reviews_per_month</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>10009999</td>
      <td>https://www.airbnb.com/rooms/10009999</td>
      <td>Horto flat with small garden</td>
      <td>One bedroom + sofa-bed in quiet and bucolic ne...</td>
      <td>Lovely one bedroom + sofa-bed in the living ro...</td>
      <td>One bedroom + sofa-bed in quiet and bucolic ne...</td>
      <td>This charming ground floor flat is located in ...</td>
      <td>There췂s a table in the living room now, that d...</td>
      <td>Easy access to transport (bus, taxi, car) and ...</td>
      <td></td>
      <td>I췂ll be happy to help you with any doubts, tip...</td>
      <td>I just hope the guests treat the space as they...</td>
      <td>Apartment</td>
      <td>Entire home/apt</td>
      <td>Real Bed</td>
      <td>2</td>
      <td>1125</td>
      <td>flexible</td>
      <td>2019-02-11 05:00:00</td>
      <td>2019-02-11 05:00:00</td>
      <td>4</td>
      <td>1</td>
      <td>2.0</td>
      <td>0</td>
      <td>1.0</td>
      <td>[Wifi, Wheelchair accessible, Kitchen, Free pa...</td>
      <td>317.00</td>
      <td>1492.00</td>
      <td>4849.00</td>
      <td>187.00</td>
      <td>0.00</td>
      <td>1</td>
      <td>{'thumbnail_url': '', 'medium_url': '', 'pictu...</td>
      <td>{'host_id': '1282196', 'host_url': 'https://ww...</td>
      <td>{'street': 'Rio de Janeiro, Rio de Janeiro, Br...</td>
      <td>{'availability_30': 0, 'availability_60': 0, '...</td>
      <td>{}</td>
      <td>[]</td>
      <td>NaT</td>
      <td>NaT</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1001265</td>
      <td>https://www.airbnb.com/rooms/1001265</td>
      <td>Ocean View Waikiki Marina w/prkg</td>
      <td>A short distance from Honolulu's billion dolla...</td>
      <td>Great studio located on Ala Moana across the s...</td>
      <td>A short distance from Honolulu's billion dolla...</td>
      <td>You can breath ocean as well as aloha.</td>
      <td></td>
      <td>Honolulu does have a very good air conditioned...</td>
      <td>Pool, hot tub and tennis</td>
      <td>We try our best at creating, simple responsive...</td>
      <td>The general welfare and well being of all the ...</td>
      <td>Condominium</td>
      <td>Entire home/apt</td>
      <td>Real Bed</td>
      <td>3</td>
      <td>365</td>
      <td>strict_14_with_grace_period</td>
      <td>2019-03-06 05:00:00</td>
      <td>2019-03-06 05:00:00</td>
      <td>2</td>
      <td>1</td>
      <td>1.0</td>
      <td>96</td>
      <td>1.0</td>
      <td>[TV, Cable TV, Wifi, Air conditioning, Pool, K...</td>
      <td>115.00</td>
      <td>650.00</td>
      <td>2150.00</td>
      <td>100.00</td>
      <td>0.00</td>
      <td>1</td>
      <td>{'thumbnail_url': '', 'medium_url': '', 'pictu...</td>
      <td>{'host_id': '5448114', 'host_url': 'https://ww...</td>
      <td>{'street': 'Honolulu, HI, United States', 'sub...</td>
      <td>{'availability_30': 16, 'availability_60': 46,...</td>
      <td>{'review_scores_accuracy': 9, 'review_scores_c...</td>
      <td>[{'_id': '4765259', 'date': 2013-05-24 04:00:0...</td>
      <td>2013-05-24 04:00:00</td>
      <td>2019-02-07 05:00:00</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10021707</td>
      <td>https://www.airbnb.com/rooms/10021707</td>
      <td>Private Room in Bushwick</td>
      <td>Here exists a very cozy room for rent in a sha...</td>
      <td></td>
      <td>Here exists a very cozy room for rent in a sha...</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td>Apartment</td>
      <td>Private room</td>
      <td>Real Bed</td>
      <td>14</td>
      <td>1125</td>
      <td>flexible</td>
      <td>2019-03-06 05:00:00</td>
      <td>2019-03-06 05:00:00</td>
      <td>1</td>
      <td>1</td>
      <td>1.0</td>
      <td>1</td>
      <td>1.5</td>
      <td>[Internet, Wifi, Air conditioning, Kitchen, Bu...</td>
      <td>40.00</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.00</td>
      <td>1</td>
      <td>{'thumbnail_url': '', 'medium_url': '', 'pictu...</td>
      <td>{'host_id': '11275734', 'host_url': 'https://w...</td>
      <td>{'street': 'Brooklyn, NY, United States', 'sub...</td>
      <td>{'availability_30': 0, 'availability_60': 0, '...</td>
      <td>{'review_scores_accuracy': 10, 'review_scores_...</td>
      <td>[{'_id': '61050713', 'date': 2016-01-31 05:00:...</td>
      <td>2016-01-31 05:00:00</td>
      <td>2016-01-31 05:00:00</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10030955</td>
      <td>https://www.airbnb.com/rooms/10030955</td>
      <td>Apt Linda Vista Lagoa - Rio</td>
      <td>Quarto com vista para a Lagoa Rodrigo de Freit...</td>
      <td></td>
      <td>Quarto com vista para a Lagoa Rodrigo de Freit...</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td>Apartment</td>
      <td>Private room</td>
      <td>Real Bed</td>
      <td>1</td>
      <td>1125</td>
      <td>flexible</td>
      <td>2019-02-11 05:00:00</td>
      <td>2019-02-11 05:00:00</td>
      <td>2</td>
      <td>1</td>
      <td>1.0</td>
      <td>0</td>
      <td>2.0</td>
      <td>[TV, Cable TV, Internet, Wifi, Air conditionin...</td>
      <td>701.00</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>250.00</td>
      <td>0.00</td>
      <td>1</td>
      <td>{'thumbnail_url': '', 'medium_url': '', 'pictu...</td>
      <td>{'host_id': '51496939', 'host_url': 'https://w...</td>
      <td>{'street': 'Rio de Janeiro, Rio de Janeiro, Br...</td>
      <td>{'availability_30': 28, 'availability_60': 58,...</td>
      <td>{}</td>
      <td>[]</td>
      <td>NaT</td>
      <td>NaT</td>
      <td>1000.00</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1003530</td>
      <td>https://www.airbnb.com/rooms/1003530</td>
      <td>New York City - Upper West Side Apt</td>
      <td></td>
      <td>Murphy bed, optional second bedroom available....</td>
      <td>Murphy bed, optional second bedroom available....</td>
      <td>Great neighborhood - many terrific restaurants...</td>
      <td>My cat, Samantha, are in and out during the su...</td>
      <td>Conveniently located near 1, 2, 3, B &amp; C subwa...</td>
      <td>New York City!</td>
      <td></td>
      <td>No smoking is permitted in the apartment. All ...</td>
      <td>Apartment</td>
      <td>Private room</td>
      <td>Real Bed</td>
      <td>12</td>
      <td>360</td>
      <td>strict_14_with_grace_period</td>
      <td>2019-03-07 05:00:00</td>
      <td>2019-03-07 05:00:00</td>
      <td>2</td>
      <td>1</td>
      <td>1.0</td>
      <td>70</td>
      <td>1.0</td>
      <td>[Internet, Wifi, Air conditioning, Kitchen, El...</td>
      <td>135.00</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>135.00</td>
      <td>0.00</td>
      <td>1</td>
      <td>{'thumbnail_url': '', 'medium_url': '', 'pictu...</td>
      <td>{'host_id': '454250', 'host_url': 'https://www...</td>
      <td>{'street': 'New York, NY, United States', 'sub...</td>
      <td>{'availability_30': 0, 'availability_60': 0, '...</td>
      <td>{'review_scores_accuracy': 10, 'review_scores_...</td>
      <td>[{'_id': '4351675', 'date': 2013-04-29 04:00:0...</td>
      <td>2013-04-29 04:00:00</td>
      <td>2018-08-12 04:00:00</td>
      <td>0.00</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>
```



</HTMLOutputBlock>

## 游닌 Via Web Scrapping
